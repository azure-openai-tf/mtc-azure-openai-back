"""
@created_by ayaan
@created_at 2023.05.08
"""
import os
import requests
from collections import OrderedDict
from azure.core.credentials import AzureKeyCredential
from azure.search.documents.indexes.aio import SearchIndexerClient

# LangCahin & OpenAI íŒ¨í‚¤ì§€
import openai
from langchain.chat_models import AzureChatOpenAI
# from langchain.vectorstores import Chroma
from langchain.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.embeddings import OpenAIEmbeddings

from dotenv import load_dotenv

load_dotenv()


class AzureOpenAIUtils:
    """Azure OpenAI Utilities"""

    azure_search_key = os.environ.get("SEARCH_KEY")
    azure_search_endpoint = os.environ.get("SEARCH_ENDPOINT")
    azure_search_api_version = "2021-04-30-preview"
    azure_openai_key = os.environ.get("OPEN_AI_KEY")
    azure_openai_endpoint = os.environ.get("OPEN_AI_ENDPOINT")
    azure_openai_api_version = "2023-03-15-preview"

    def __init__(self):
        self.headers = {"Content-Type": "application/json", "api-key": self.azure_search_key}
        self.params = {"api-version": self.azure_search_api_version}  # ìµœì‹  Preview ë²„ì „ 2021-04-30-preview

        self.cognitive_search_credential = AzureKeyCredential(self.azure_search_key)

        # Azure OpenAI ì—°ê²° í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        openai.api_type = "azure"  # ì¤‘ìš”!
        openai.api_version = self.azure_openai_api_version  # ìµœì‹  preview ë²„ì „ 2023-03-15-preview
        openai.api_base = self.azure_openai_endpoint
        openai.api_key = self.azure_openai_key

    async def cognitive_search_run_indexer(self, index_name):
        """cognitive_search_run_indexer"""
        search_indexer_client = SearchIndexerClient(self.azure_search_endpoint, self.cognitive_search_credential)
        # indexer = await search_indexer_client.get_indexer(indexer_name)
        await search_indexer_client.run_indexer(index_name)

        await search_indexer_client.close()
        print("index ì—…ë°ì´íŠ¸ ì™„ë£Œ")

    async def cognitive_search_get_indexer_status(self, indexer_name):
        """cognitive_search_get_indexer_status"""
        search_indexer_client = SearchIndexerClient(self.azure_search_endpoint, self.cognitive_search_credential)
        # indexer = await search_indexer_client.get_indexer(indexer_name)
        return await search_indexer_client.get_indexer_status(indexer_name)

    async def execute_openai(self, question, index_name, vector_store_name):
        """Excute OpenAI"""
        # ì§ˆë¬¸ ì„¤ì •
        # QUESTION = ' Azure ê´€ë¦¬ì ìê²©ì¦ì¤‘ì— ì–´ë–¤ ìê²©ì¦ì´ ìˆëŠ”ì§€ ì•„ì£¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜' --> ì´ ë©”ì‹œì§€ë¥¼ ë„£ìœ¼ë©´ ì—ëŸ¬ê°€ ë‚œë‹¤..
        question = " Azure ê´€ë¦¬ìê°€ ë˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ”ì§€ ìê²©ì¦ë„ ì„¤ëª…í•´ì£¼ê³  ì•Œë ¤ì¤˜"

        # Azure Cognitive Search REST API í˜¸ì¶œ(get)
        url = self.azure_search_endpoint + "/indexes/" + index_name + "/docs"
        params = {
            "api-version": self.azure_search_api_version,
            "search": question,
            "queryLanguage": "en-US",
            "queryType": "semantic",
            "semanticConfiguration": "semantic-config",
            "select": "*",
            "$count": "true",
            "speller": "lexicon",
            "$top=5": "5",
            "answers": "extractive|count-3",
            "captions": "extractive|highlight-false",
        }

        resp = requests.get(url, params=params, headers=self.headers)
        search_results = resp.json()  # ê²°ê³¼ê°’

        print("API í˜¸ì¶œ ê²°ê³¼ :", resp.status_code)

        # print(search_results)
        # semantic-config ì„¤ì • ê¼­ í•„ìš”
        # print(search_results)
        print(f"ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: {search_results['@odata.count']}, : ìƒìœ„ ë¬¸ì„œ ìˆ˜: {len(search_results['value'])}")

        # display(HTML("<h4>ìƒìœ„ ì—°ê´€ ë¬¸ì„œ</h4>"))

        file_content = OrderedDict()
        for result in search_results["value"]:
            # print('result : ' , result)
            # print(result['@search.rerankerScore'])
            # print('file_content : ', file_content)
            print("path : ", result["metadata_storage_path"])
            if result["@search.rerankerScore"] > 0.04:  # Semantic Search ìµœëŒ€ ì ìˆ˜ 4ì , ìƒìœ„ 40%
                # print('##########################################################################################################')
                # display(HTML("<h1>" + str(result["metadata_storage_name"]) + ", score: " + str(result["@search.rerankerScore"]) + "</h1>"))
                # display(HTML(result["@search.captions"][0]["text"]))
                file_content[result["metadata_storage_path"]] = {
                    "chunks": result["pages"][:1],
                    "caption": result["@search.captions"][0]["text"],
                    "score": result["@search.rerankerScore"],
                    "file_name": result["metadata_storage_name"],
                }
                # print('file_content : ', file_content)

        # AzureOpenAI Service ì—°ê²°
        # ë¬¸ì„œ ë¶„í• 
        docs = []
        for key, value in file_content.items():
            # print('key : ' , key , '\t value : ' , value)
            # print(value['chunks'])
            for page in value["chunks"]:
                docs.append(Document(page_content=page, metadata={"source": value["file_name"]}))
        print("Number of chunks:", len(docs))
        # print('#####################################################')
        # print('docs : ' , docs)

        # Embedding ëª¨ë¸ ìƒì„±
        # ì•„ë˜ì†ŒìŠ¤ì—ì„œ chunk_size=1 ì´ ì•„ë‹Œ ë‹¤ë¥¸ ê°’ì„ ë„£ìœ¼ë©´ ë‹¤ìŒ ì†ŒìŠ¤ì—ì„œ ì—ëŸ¬ê°€ ë‚œë‹¤.
        embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002", chunk_size=1, openai_api_key=self.azure_openai_key
        )  # Azure OpenAI embedding ì‚¬ìš©ì‹œ ì£¼ì˜
        
        # Vector Store ìƒì„±
        persist_directory = "db"
        vector_store = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=persist_directory)
        vector_store = Chroma.from_documents(docs, embeddings)
        if vector_store_name == 'FAISS':
            vector_store = FAISS.from_documents(docs, embeddings)

        # LangChainğŸ¦œ & Azure GPTğŸ¤– ì—°ê²°
        # llm = AzureChatOpenAI(deployment_name='gpt-35-turbo',  openai_api_key=AZURE_OPENAI_KEY, openai_api_base=AZURE_OPENAI_ENDPOINT, openai_api_version=AZURE_OPENAI_API_VERSION,
        llm = AzureChatOpenAI(
            deployment_name="chat",
            openai_api_key=self.azure_openai_key,
            openai_api_base=self.azure_openai_endpoint,
            openai_api_version=self.azure_openai_api_version,
            temperature=0.0,
            max_tokens=1000,
        )

        # https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html
        qa = RetrievalQAWithSourcesChain.from_chain_type(
            llm=llm,
            # chain_type='stuff',
            chain_type="map_reduce",
            retriever=vector_store.as_retriever(),
            return_source_documents=True,
        )

        print(qa)

        result = qa({"question": question})
        # ë‹µë³€ ê¸€ììˆ˜ ì¹´ìš´íŠ¸
        # char_counts=0
        # ls_str = list(map(str,result))
        # for ls_str1 in ls_str:
        #   char_counts = char_counts + len(ls_str1)

        # print(char_counts)

        print("ì§ˆë¬¸ :", question)
        print("ë‹µë³€ :", result["answer"])
        print("ğŸ“„ ì°¸ê³  ìë£Œ :", result["sources"].replace(",", "\n"))

        return result["answer"]
